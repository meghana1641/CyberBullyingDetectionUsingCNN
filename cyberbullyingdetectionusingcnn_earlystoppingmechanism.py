# -*- coding: utf-8 -*-
"""CyberbullyingDetectionUsingCNN_EarlyStoppingMechanism.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4UmEe_qGZgB93WhEVSthDkX0PmGA1uL
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import string
import nltk
from tensorflow import keras
from sklearn.model_selection import train_test_split
from nltk.stem.wordnet import WordNetLemmatizer
import re
from tensorflow.keras.layers import Embedding,Flatten,Dense,Conv1D,MaxPooling1D
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator
import plotly.express as px
from collections import Counter
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import Sequential
from sklearn.metrics import classification_report, confusion_matrix

df = pd.read_csv("twitter_racism_parsed_dataset.csv")
df

df.info()

df.drop(['index', 'id'], inplace=True, axis=1)
df

df.loc[df['oh_label'] == 1,'Annotation'] = 'cyberbullying'
df.loc[df['oh_label'] != 1,'Annotation'] = 'not_cyberbullying'

df

df[['Annotation', 'oh_label']].value_counts()     #display the counts of unique combinations of values in the specified columns 'Annotation' and 'oh_label'

df

"""**Let's drop duplicates**"""

print(df.duplicated().sum())        # the sum of duplicated rows in the DataFrame
df=df.drop_duplicates()             #removes duplicate rows from the original DataFrame.
print(df.duplicated().sum())

"""**How long are the messages in each category?**"""

nltk.download('punkt')
#get the number of words in every tweet
def length(text):
    return len(word_tokenize(text))
df=df.copy()
df['word_count'] = df['Text'].apply(length)

fig = px.histogram(df, x="word_count", color="Annotation", title="Words in the tweet (including very long tweets)")
fig.show()
px.histogram(df[df.word_count<20], x="word_count", color="Annotation", title="Words in the tweet (excluding very long tweets)")

"""**The wordclouds for every type of cyberbullying**"""

print("Non-Cyberbullying")
text = " ".join(review for review in df[df.oh_label==0].Text.astype(str))
wordcloud = WordCloud(stopwords=STOPWORDS).generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

print("Cyberbullying")
text = " ".join(review for review in df[df.oh_label==1].Text.astype(str))
wordcloud = WordCloud(stopwords=STOPWORDS).generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""**Feature engineering and data preprocessing**"""

def remove_punct(text):
  return text.translate(str.maketrans('', '',string.punctuation))

df['no_punctuation'] = df['Text'].apply(lambda x: remove_punct(x))

df.head()

df['no_punctuation']

nltk.download('stopwords')
def lower(text):
    return text.lower()
df['no_punctuation'] = df['no_punctuation'].apply(lower)
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    new_text = []
    for el in word_tokenize(text):
        if not el in stop_words:
            new_text.append(el)
    return new_text
df['no_stopwords'] = df.no_punctuation.apply(remove_stopwords)

"""**Let's separate words and emojis.**"""

def smile_handle(word_list):
  new_word_list = []
  emoji_pattern = re.compile(r"([\U00002600-\U000027BF])|([\U0001f300-\U0001f64F])|([\U0001f680-\U0001f6FF])", flags=re.UNICODE)
  for word in word_list:
    if len(re.findall(emoji_pattern,word))!=0:
      if len(re.findall(emoji_pattern,word))!=len(word):
        new_word_list.append(re.sub(emoji_pattern,'',word))
      new_word_list.extend(re.findall(emoji_pattern,word))
    else:
      new_word_list.append(word)
  for i,el in enumerate(new_word_list):
    if type(el)==tuple:
      new_word_list[i] = el[1]
  return new_word_list

df.no_stopwords = df.no_stopwords.apply(smile_handle)

"""**Let's standartize words with the help of lemmatization**"""

nltk.download('wordnet')
def lemmatize(words):
    new_words = []
    lem = WordNetLemmatizer()
    for w in words:
        new_words.append(lem.lemmatize(w))
    return new_words

df['lemmatized'] = df.no_stopwords.apply(lemmatize)

df

"""**Let's form the vocabulary and get its length**"""

vocab = Counter()

def add_to_vocab(words):
  global vocab
  vocab.update(words)
df.lemmatized.apply(add_to_vocab)
global vocab_size
vocab_size = len(vocab)
print("Vocabulary size: ",vocab_size)

"""**Top-50 words**"""

vocab.most_common(50)

"""**Let's drop too unfrequent words!**"""

words = [key for key,val in vocab.items() if val>=3]
vocab_size = len(words)
print(vocab_size)
def remove_rare(text):
    global words
    for el in text:
        if not el in words:
            text.remove(el)
    return text

df.lemmatized = df.lemmatized.apply(remove_rare)

df

"""**Model fitting**

**train-test-split**
"""

X = df['lemmatized']
y = df['oh_label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=72,stratify=y)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**new vocabulary size**"""

vocab = Counter()
def add_to_vocab(words):
  global vocab
  vocab.update(words)
X_train.apply(add_to_vocab)
df['lemmatized'].apply(add_to_vocab)
vocab_size = len(vocab)
vocab_size

"""**For simplicity, let's convert texts to number sequences**"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

import pickle
#saving the Tokenizer
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

"""**Now let's bring the sequences to the same size**"""

global max_size
max_size = len(max(df['lemmatized'],key=lambda x:len(x)))
max_size

X_train = pad_sequences(X_train, maxlen=max_size, padding='post')

X_test = pad_sequences(X_test, maxlen=max_size, padding='post')

"""**defining model**"""

X_train

from tensorflow.keras import regularizers
def define_model(vocab_size,max_length):

# with tpu_strategy.scope():
    model = Sequential()
    model.add(Embedding(vocab_size, 100, input_length=max_size))
    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))
    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))
    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], steps_per_execution=64)
    model.summary()
    return model

import tensorflow as tf
# Create a callback for early stopping
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # or 'val_accuracy' for accuracy
    patience=3,           # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

model = define_model(vocab_size,max_size)
history = model.fit(X_train, y_train,validation_data=(X_test, y_test), epochs=50,batch_size=1024, verbose=2,callbacks=[early_stopping_callback])

"""**Model testing**"""

binary_predictions=model.predict(X_test)
results = model.evaluate(X_test, y_test,verbose=2)
results
print("Accuracy: %.2f%%" % (results[1] * 100))

import matplotlib.pyplot as plt
# Plot the training history
plt.figure(figsize=(12, 4))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper left')

plt.show()

# Convert predictions to binary labels
binary_predictions = np.round(binary_predictions).astype(int)

# Generate classification report
print("Classification Report:\n", classification_report(y_test, binary_predictions))

# Generate confusion matrix
cm = confusion_matrix(y_test, binary_predictions)

# Plot confusion matrix
plt.figure(figsize=(4, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=["Not Cyberbullying", "Cyberbullying"],
            yticklabels=["Not Cyberbullying", "Cyberbullying"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

#Testing
def test_example(example):
    # Preprocess the input example
    no_punctuation = remove_punct(example)
    text_lower = lower(no_punctuation)
    no_stopwords = remove_stopwords(text_lower)
    separate_words = smile_handle(no_stopwords)
    lemmatization = lemmatize(separate_words)
    testing_example = [lemmatization]
    testing_example = tokenizer.texts_to_sequences(testing_example)
    testing_example = pad_sequences(testing_example, maxlen=max_size, padding='post')

    # Make prediction using the model
    predict = model.predict(testing_example)
    predict = np.round(predict).astype(int)
    # Interpret the prediction
    interpretations = {
        0: "Not Cyberbullying",
        1: "Cyberbullying",
    }

    for i in interpretations.keys():
        if i == predict:
            return interpretations[i]

# Example usage
#example = "Girl bully’s as well. I’ve 2 sons that were bullied in Jr High. Both were bullied by girls. My older was bullied because he had 4ft long brown hair and a baby face. Younger was bullied cuz he hung around the nerd crowd and was an easy target. I know what u mean though! Peace"
example="@semzyxx Do you approve of your pedophile prophet raping a 9 year old girl, like it says in 7 hadith?"
prediction = test_example(example)
print("Prediction:", prediction)

import tensorflow as tf
model.save('trained_model.h5')

# Loading the model
loaded_model = tf.keras.models.load_model('trained_model.h5')

#Testing
def test_example(example):
    # Preprocess the input example
    no_punctuation = remove_punct(example)
    text_lower = lower(no_punctuation)
    no_stopwords = remove_stopwords(text_lower)
    separate_words = smile_handle(no_stopwords)
    lemmatization = lemmatize(separate_words)
    testing_example = [lemmatization]
    testing_example = tokenizer.texts_to_sequences(testing_example)
    testing_example = pad_sequences(testing_example, maxlen=max_size, padding='post')

    # Make prediction using the model
    predict = loaded_model.predict(testing_example)
    predict = np.round(predict).astype(int)
    # Interpret the prediction
    interpretations = {
        0: "Not Cyberbullying",
        1: "Cyberbullying",
    }

    for i in interpretations.keys():
        if i == predict:
            return interpretations[i]

example="@Skawtnyc @athenahollow @twoscooters i don't tend to talk about it much. :P personal info."
prediction = test_example(example)
print("Prediction:", prediction)